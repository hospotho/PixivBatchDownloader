# 自动合并系列小说的测试记录

以下压力测试是按时间倒序排列的。均启用了这些功能：
- 自动合并系列小说
- 不再单独下载系列里的小说
- 在小说里保存元数据
- 下载小说的封面图片
- 下载小说里的内嵌图片

## 测试 5

该功能已经完成，并且添加了分割文件功能来解决单个大体积 EPUB 文件创建失败的问题。

进行大量抓取的压力测试。

其实这个压力测试之前就完整的进行过一次，但保存格式是 txt，这次是保存为 epub，两次合起来的持续工作时间有 27 个小时。

搜索“R-18G”的所有小说：
https://www.pixiv.net/tags/R-18G/novels?mode=r18&s_mode=s_tag&work_lang=zh-cn&ai_type=1

抓取 100 页（共 3000 个小说）。

抓取开始的时间：23:05
抓取完成的时间：14:21
共用时：14 小时 26 分钟 = 866 分钟

发送了约 22960 个请求，其中小说的封面图、内嵌图片约 11920 个，API 的请求约 10916 个。平均每分钟 26.5 个请求。没有触发 429 错误，也没有被警告。这说明当前设置（添加了间隔时间）在长时间执行自动合并系列小说的任务时是安全的。

有 726 个系列小说，里面一共包含 7292 篇小说，平均每个系列有 10 个小说。

下载的 EPUB 文件总体积 8.63 GB，单独保存的封面图共 509 MB。里面一共包含约 2717 篇小说，平均每个系列有 14 个小说。

这 100 页的 3000 个小说里最后只有 1095 个抓取结果，说明有 1900 个小说属于系列小说。

--------

**内存和体积：**

从 Chrome 的任务管理器里看，抓取完成后该页面的内存占用为 970 MB，相比于初始内存增加了约 800 MB。
从开发者工具里查看，堆内存是 400 MB 出头，比初始值增加了约 250 MB，下载器增加的堆内存主要是两部分：
- cache 对象缓存了 6000 个小说的完整 JSON 数据，其内存占用为 204 MiB，平均每篇小说的数据占用 35 KiB 内存。
- 1095 个抓取结果（store.result）的内存占用为 32 MiB，平均每篇小说占用 30 KiB 内存。

这 1095 个抓取结果导出的 JSON 文件体积是 44.68 MiB，平均每个小说的抓取结果的体积为 42 KiB。

单篇小说的原始数据最大的是一篇 590 KB 的（https://www.pixiv.net/n/23349307），接下来有 4 个超过 400 KB 的，5 个超过 300 KB 的，其余的就更小了。

## 测试 4

该功能已经基本完成，由于上一次测试依然被警告，所以我再次加大了下载文件时的间隔时间（抓取时的间隔已经够了，因为一直没触发 429 错误）。这次没被警告。

搜索“芙宁娜”的所有小说：
https://www.pixiv.net/tags/%E3%83%95%E3%83%AA%E3%83%BC%E3%83%8A/novels

全部抓取（共 1147 个小说）。

抓取开始的时间：19:43
抓取完成的时间：0:14
共用时：4 小时 31 分钟 = 271 分钟

发送了约 7300 个请求，其中小说的封面图、内嵌图片约 3280 个，API 的请求约 4000 个。平均每分钟 27 个请求。没有触发 429 错误，也没有被警告。这说明当前设置（添加了间隔时间）在长时间执行自动合并系列小说的任务时是安全的。

有 198 个系列小说，总体积 1.06 GB。里面一共包含约 2717 篇小说，平均每个系列有 14 个小说。

列表页的 1147 个小说里最后只有 498 个抓取结果，说明有 649 个小说属于系列小说。

抓取完毕后，该网页内存占用为 614 MB。

## 测试 3

该功能已经基本完成，但当前的间隔时间导致被警告。截止到现在一共被警告了 3 次。再之后的测试里由于加大了间隔时间，没有再被警告。

搜索“碧蓝档案”标签里的 R-18 小说：
https://www.pixiv.net/tags/%E3%83%96%E3%83%AB%E3%83%BC%E3%82%A2%E3%83%BC%E3%82%AB%E3%82%A4%E3%83%96/novels?mode=r18

抓取 30 页（900 个小说），并启用“自动合并系列小说”。

抓取开始的时间：18:08
抓取完成的时间：1:53
共用时：7 小时 45 分钟 = 465 分钟

发送了约 13500 个请求，其中小说的封面图、内嵌图片约 6500 个，API 的请求约 6500 个。平均每分钟 30 个请求。没有触发 429 错误，当时也没有被警告，但 7 个小时候收到了警告信。这可能是因为下载的文件数量很多导致的。

有 170 个系列小说，总体积 1.38 GB。里面一共包含约 5610 篇小说（通过统计日志里下载小说的封面图片数量，并且已排除了系列封面图片），平均每个系列有 33 个小说。注意这比下面两个测试里的小说平均数量高很多。这说明搜索不同标签时，差别可能很大。

列表页的 900 个小说里最后只有 406 个抓取结果，说明有 494 个小说属于系列小说。

抓取完毕后，该网页内存占用为 1.7 GB。我在保存 EPUB 小说后已经释放了 blobURL，可能是因为图片资源没有释放导致的。不过这个问题之后会解决，因为之后我打算让合并小说时的文件也交给浏览器保存，在 background 脚本里会释放它们的 blobURL。

## 测试 2

该功能已经基本完成，测试大量抓取时会不会被警告。结果被警告了。

搜索“原神”标签里的 R-18、简体中文、非 AI 生成的小说：
https://www.pixiv.net/tags/%E5%8E%9F%E7%A5%9E/novels?mode=r18&s_mode=s_tag&work_lang=zh-cn&ai_type=1

抓取 30 页（900 个小说），并启用“自动合并系列小说”。

抓取开始的时间：0:32
抓取完成的时间：2:30
共用时：120 分钟

发送了 4200 个请求，其中小说的封面图、内嵌图片约 2000 个，API 的请求约 2200 个。平均每分钟 35 个请求。没有触发 429 错误，也没有被警告。

有 143 个系列小说，总体积接近 900 MB。里面一共包含约 1540 篇小说（通过统计日志里下载小说的封面图片数量，并且已排除了系列封面图片），平均每个系列有 10.7 个小说。

列表页的 900 个小说里最后只有 348 个抓取结果，说明有 552 个小说属于系列小说。

抓取完成后页面内存占用为 800 MB，关闭后重新打开只有 260 MB。尚不清楚增加的内存都是些什么数据。虽然日志很多（2700 条），但是删除日志元素、以及刷新页面后内存占用依然没什么变化，所以日志占用的内存应该不是主要原因。抓取结果才 20 MB，已缓存的数据可能有 150 MB 左右。可能是图片的 blobURL 没有释放导致的。

## 测试 1

该功能只初步完成，而且很多请求之间没有添加间隔时间，导致被警告了。

在虚拟主播的 R-18 小说页面里抓取 30 页（900 个小说），并启用“自动合并系列小说”。

仅用时 80 分钟就完成了抓取，共发送了约 4500 次请求，其中 2000 个图片，2500 个 API，平均每分钟 56 个请求。

抓取时只触发了一次 429 错误，但可能是因为下载文件（封面、插画）比较频繁（因为基本没有间隔时间），还是收到了警告。

这次有 135 个系列小说，里面共含有 1864 篇小说，平均每个系列含有 14 篇小说。
